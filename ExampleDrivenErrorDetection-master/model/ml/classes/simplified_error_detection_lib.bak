# -*- coding: utf-8 -*-
import numpy as np
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
from ml.active_learning.library import *
from ml.Word2VecFeatures.Word2VecFeatures import Word2VecFeatures
from ml.features.ActiveCleanFeatures import ActiveCleanFeatures
from ml.features.ValueCorrelationFeatures import ValueCorrelationFeatures
from ml.features.BoostCleanMetaFeatures import BoostCleanMetaFeatures
from ml.active_learning.classifier.SimplifiedXGBoostClassifier import SimplifiedXGBoostClassifier
import time
import pickle


def add_lstm_features(data, use_lstm_only, all_matrix_train, feature_name_list):
    """
    添加LSTM特征
    """
    lstm_path = ""
    if data.name == 'Flight HoloClean':
        lstm_path = Config.get('lstm.folder') + "/Flights/last/"
    elif data.name == 'HospitalHoloClean':
        lstm_path = Config.get('lstm.folder') + "/HospitalHoloClean/last/"
    elif data.name == 'BlackOakUppercase':
        lstm_path = Config.get('lstm.folder') + "/BlackOakUppercase/last/"
    else:
        print "Warning: No LSTM model available for this dataset, skipping LSTM features"
        return all_matrix_train, None, feature_name_list

    all_matrix_train_deep = read_compressed_deep_features(lstm_path)
    all_matrix_test = None
    feature_name_list_deep = ['deep ' + str(dfeature) for dfeature in xrange(all_matrix_train_deep.shape[1])]

    if use_lstm_only:
        all_matrix_train = all_matrix_train_deep
        feature_name_list = feature_name_list_deep
    else:
        all_matrix_train = hstack((all_matrix_train, all_matrix_train_deep)).tocsr()
        feature_name_list.extend(feature_name_list_deep)

    return all_matrix_train, all_matrix_test, feature_name_list


def create_features_matrix(dataSet, train_indices, test_indices,
                          ngrams=1, runSVD=False, is_word=False, use_tf_idf=True,
                          use_metadata=True, use_metadata_only=False,
                          use_lstm=False, use_lstm_only=False,
                          use_word2vec=False, use_word2vec_only=False, w2v_size=100,
                          use_active_clean=False, use_activeclean_only=False,
                          use_cond_prob=False, use_cond_prob_only=False,
                          use_boostclean_metadata=False, use_boostclean_metadata_only=False):
    """
    创建特征矩阵
    """
    # 基础特征
    all_matrix_train, all_matrix_test, feature_name_list = create_features(
        dataSet, train_indices, test_indices, ngrams, runSVD, is_word, use_tf_idf)

    # 元数据特征
    if use_metadata:
        all_matrix_train, all_matrix_test, feature_name_list = add_metadata_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_metadata_only)

    # LSTM特征
    if use_lstm:
        all_matrix_train, all_matrix_test, feature_name_list = add_lstm_features(
            dataSet, use_lstm_only, all_matrix_train, feature_name_list)

    # Word2Vec特征
    if use_word2vec:
        w2v_features = Word2VecFeatures(vector_size=w2v_size)
        all_matrix_train, all_matrix_test, feature_name_list = w2v_features.add_word2vec_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_word2vec_only)

    # ActiveClean特征
    if use_active_clean:
        ac_features = ActiveCleanFeatures()
        all_matrix_train, all_matrix_test, feature_name_list = ac_features.add_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_activeclean_only)

    # 条件概率特征
    if use_cond_prob:
        ac_features = ValueCorrelationFeatures()
        all_matrix_train, all_matrix_test, feature_name_list = ac_features.add_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_cond_prob_only)

    # BoostClean元数据特征
    if use_boostclean_metadata:
        ac_features = BoostCleanMetaFeatures()
        all_matrix_train, all_matrix_test, feature_name_list = ac_features.add_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_boostclean_metadata_only)

    # 检查是否所有特征矩阵都为空
    if all_matrix_train is None or (hasattr(all_matrix_train, 'shape') and all_matrix_train.shape[0] == 0):
        print "Warning: No features generated, creating default feature matrix"
        # 创建一个简单的默认特征矩阵，避免连接空数组的问题
        from scipy.sparse import csr_matrix
        import numpy as np
        
        # 为每个数据点创建一个简单的特征
        num_train = len(train_indices) if train_indices is not None else dataSet.shape[0]
        num_test = len(test_indices) if test_indices is not None else dataSet.shape[0]
        
        all_matrix_train = csr_matrix(np.ones((num_train, 1)))
        all_matrix_test = csr_matrix(np.ones((num_test, 1)))
        feature_name_list = ['default_feature']

    return all_matrix_train, all_matrix_test, feature_name_list


def prepare_data_for_training(train_dataSet, test_dataSet=None, train_fraction=1.0):
    """
    为训练准备数据
    
    参数:
    - train_dataSet: 训练数据集对象
    - test_dataSet: 测试数据集对象（可选）
    - train_fraction: 训练数据比例（用于分割训练集）
    """
    
    # 分割训练数据索引
    if train_fraction < 1.0:
        train_indices, val_indices = split_data_indices(train_dataSet, train_fraction, fold_number=0)
    else:
        train_indices = np.arange(train_dataSet.shape[0])
        val_indices = []
    
    # 如果没有提供测试集，使用训练集的验证部分作为测试集
    if test_dataSet is None:
        test_indices = val_indices
        test_dataSet = train_dataSet
    else:
        test_indices = np.arange(test_dataSet.shape[0])
    
    # 为训练集创建特征
    all_matrix_train, _, feature_name_list = create_features_matrix(
        train_dataSet, train_indices, test_indices)
    
    try:
        feature_matrix_train = all_matrix_train.tocsr()
    except:
        feature_matrix_train = all_matrix_train

    # 为测试集创建特征
    if test_dataSet is not train_dataSet:
        all_matrix_test, _, _ = create_features_matrix(
            test_dataSet, train_indices, test_indices)
    else:
        # 如果测试集是训练集的一部分，使用相同的特征矩阵
        all_matrix_test = all_matrix_train
    
    try:
        feature_matrix_test = all_matrix_test.tocsr()
    except:
        feature_matrix_test = all_matrix_test

    # 为训练集每列创建特征矩阵并添加列标识
    feature_matrix_train_per_column = []
    train_debugging_ids = []
    
    for column_i in xrange(train_dataSet.shape[1]):
        # 添加列标识的one-hot编码
        one_hot_part = np.zeros((train_dataSet.shape[0], train_dataSet.shape[1]))
        one_hot_part[:, column_i] = 1
        
        # 将列标识特征与原有特征合并
        # 检查feature_matrix_train是否为空
        if feature_matrix_train is None or (hasattr(feature_matrix_train, 'shape') and feature_matrix_train.shape[0] == 0):
            # 如果特征矩阵为空，只使用列标识特征
            from scipy.sparse import csr_matrix
            feature_matrix_train_per_column.append(csr_matrix(one_hot_part))
        else:
            feature_matrix_train_per_column.append(hstack((feature_matrix_train, one_hot_part)).tocsr())
        
        # 记录调试信息
        for row in xrange(train_dataSet.shape[0]):
            train_debugging_ids.append((row, column_i))

    # 垂直堆叠训练集所有列的特征矩阵
    all_columns_feature_matrix_train = vstack(feature_matrix_train_per_column)
    
    # 为测试集每列创建特征矩阵并添加列标识
    feature_matrix_test_per_column = []
    test_debugging_ids = []
    
    for column_i in xrange(test_dataSet.shape[1]):
        # 添加列标识的one-hot编码
        one_hot_part = np.zeros((test_dataSet.shape[0], test_dataSet.shape[1]))
        one_hot_part[:, column_i] = 1
        
        # 将列标识特征与原有特征合并
        # 检查feature_matrix_test是否为空
        if feature_matrix_test is None or (hasattr(feature_matrix_test, 'shape') and feature_matrix_test.shape[0] == 0):
            # 如果特征矩阵为空，只使用列标识特征
            from scipy.sparse import csr_matrix
            feature_matrix_test_per_column.append(csr_matrix(one_hot_part))
        else:
            feature_matrix_test_per_column.append(hstack((feature_matrix_test, one_hot_part)).tocsr())
        
        # 记录调试信息
        for row in xrange(test_dataSet.shape[0]):
            test_debugging_ids.append((row, column_i))

    # 垂直堆叠测试集所有列的特征矩阵
    all_columns_feature_matrix_test = vstack(feature_matrix_test_per_column)
    
    # 创建训练集标签数组（将所有列的标签展平）
    ground_truth_train_array = train_dataSet.matrix_is_error[train_indices, 0]
    for column_i in xrange(1, train_dataSet.shape[1]):
        ground_truth_train_array = np.concatenate((ground_truth_train_array, 
                                                   train_dataSet.matrix_is_error[train_indices, column_i]))

    # 创建测试集标签数组（将所有列的标签展平）
    if test_dataSet is train_dataSet:
        ground_truth_test_array = train_dataSet.matrix_is_error[test_indices, 0]
        for column_i in xrange(1, train_dataSet.shape[1]):
            ground_truth_test_array = np.concatenate((ground_truth_test_array, 
                                                     train_dataSet.matrix_is_error[test_indices, column_i]))
    else:
        ground_truth_test_array = test_dataSet.matrix_is_error[test_indices, 0]
        for column_i in xrange(1, test_dataSet.shape[1]):
            ground_truth_test_array = np.concatenate((ground_truth_test_array, 
                                                     test_dataSet.matrix_is_error[test_indices, column_i]))

    return (all_columns_feature_matrix_train, all_columns_feature_matrix_test, 
            ground_truth_train_array, ground_truth_test_array, 
            feature_name_list, train_debugging_ids, test_debugging_ids)


def train_and_evaluate_error_detection_model(train_dataSet, test_dataSet=None, classifier_model=None,
                                           ngrams=1, runSVD=False, is_word=False,
                                           use_metadata=True, use_metadata_only=False,
                                           use_lstm=False, use_lstm_only=False,
                                           use_word2vec=False, use_word2vec_only=False, w2v_size=100,
                                           use_active_clean=False, use_activeclean_only=False,
                                           use_cond_prob=False, use_cond_prob_only=False,
                                           use_boostclean_metadata=False, use_boostclean_metadata_only=False,
                                           train_fraction=1.0, model_save_path=None, use_cv=False):
    """
    简化版错误检测模型训练和评估
    
    参数:
    - train_dataSet: 训练数据集对象
    - test_dataSet: 测试数据集对象（可选）
    - classifier_model: 分类器模型类
    - 各种特征开关参数
    - train_fraction: 训练数据比例
    - model_save_path: 模型保存路径（可选）
    - use_cv: 是否使用交叉验证（可选）
    
    返回:
    - 包含评估指标的字典
    """
    
    if classifier_model is None:
        classifier_model = SimplifiedXGBoostClassifier
    
    # 准备训练数据
    print "Preparing training data..."
    (all_columns_feature_matrix_train, all_columns_feature_matrix_test,
     ground_truth_train_array, ground_truth_test_array,
     feature_name_list, train_debugging_ids, test_debugging_ids) = prepare_data_for_training(
        train_dataSet, test_dataSet, train_fraction)
    
    print "Train feature matrix shape: " + str(all_columns_feature_matrix_train.shape)
    print "Test feature matrix shape: " + str(all_columns_feature_matrix_test.shape)
    print "Train ground truth array length: " + str(len(ground_truth_train_array))
    print "Test ground truth array length: " + str(len(ground_truth_test_array))
    
    # 初始化分类器
    classifier = classifier_model(feature_names=feature_name_list, balance=True)
    
    # 训练模型
    print "Training model..."
    start_time = time.time()
    
    # 使用训练集训练模型
    classifier.train(all_columns_feature_matrix_train, ground_truth_train_array, use_cv=use_cv)
    
    training_time = time.time() - start_time
    print "Training completed in {0:.2f} seconds".format(training_time)
    
    # 保存模型
    if model_save_path:
        print "Saving model to " + model_save_path
        classifier.save_model(model_save_path)
    
    # 在测试集上进行预测
    print "Making predictions on test set..."
    probability_prediction = classifier.predict_proba(all_columns_feature_matrix_test)
    class_prediction = classifier.predict(all_columns_feature_matrix_test)
    
    # 计算评估指标
    f1 = f1_score(ground_truth_test_array, class_prediction)
    precision = precision_score(ground_truth_test_array, class_prediction)
    recall = recall_score(ground_truth_test_array, class_prediction)
    
    # 按列计算评估指标
    per_column_metrics = {}
    test_dataset = test_dataSet if test_dataSet is not None else train_dataSet
    
    for col_i in xrange(test_dataset.shape[1]):
        start_idx = col_i * test_dataset.shape[0]
        end_idx = (col_i + 1) * test_dataset.shape[0]
        
        col_true = ground_truth_test_array[start_idx:end_idx]
        col_pred = class_prediction[start_idx:end_idx]
        
        per_column_metrics[test_dataset.clean_pd.columns[col_i]] = {
            'f1': f1_score(col_true, col_pred),
            'precision': precision_score(col_true, col_pred),
            'recall': recall_score(col_true, col_pred)
        }
    
    # 获取特征重要性
    feature_importance = classifier.get_feature_importance()
    
    # 返回结果
    result = {
        'overall': {
            'f1': f1,
            'precision': precision,
            'recall': recall,
            'training_time': training_time,
            'total_train_samples': len(ground_truth_train_array),
            'total_test_samples': len(ground_truth_test_array)
        },
        'per_column': per_column_metrics,
        'predictions': class_prediction,
        'ground_truth': ground_truth_test_array,
        'probabilities': probability_prediction,
        'feature_importance': feature_importance,
        'classifier': classifier
    }
    
    return result


def run_error_detection_experiment(train_dataSet, test_dataSet=None, classifier_model=None, **kwargs):
    """
    运行错误检测实验
    
    参数:
    - train_dataSet: 训练数据集对象
    - test_dataSet: 测试数据集对象（可选）
    - classifier_model: 分类器模型类
    - **kwargs: 其他参数传递给train_and_evaluate_error_detection_model
    """
    
    print "Running error detection experiment on " + train_dataSet.name
    if test_dataSet:
        print "Test dataset: " + testData.name
    print "=" * 50
    
    # 训练和评估模型
    result = train_and_evaluate_error_detection_model(
        train_dataSet, test_dataSet, classifier_model, **kwargs)
    
    # 打印结果
    print "\nOverall Results:"
    print "F1 Score: {0:.4f}".format(result['overall']['f1'])
    print "Precision: {0:.4f}".format(result['overall']['precision'])
    print "Recall: {0:.4f}".format(result['overall']['recall'])
    print "Training Time: {0:.2f} seconds".format(result['overall']['training_time'])
    print "Total Train Samples: " + str(result['overall']['total_train_samples'])
    print "Total Test Samples: " + str(result['overall']['total_test_samples'])
    
    print "\nPer-Column Results:"
    for col_name, metrics in result['per_column'].items():
        print col_name + ": F1={0:.4f}, Precision={1:.4f}, Recall={2:.4f}".format(metrics['f1'], metrics['precision'], metrics['recall'])
    
    return result


def detect_errors_in_new_data(model_path, new_dataSet, classifier_model=None):
    """
    使用已训练的模型对新数据进行错误检测
    
    参数:
    - model_path: 已训练模型的路径
    - new_dataSet: 新数据集对象
    - classifier_model: 分类器模型类（需要与训练时使用的模型一致）
    
    返回:
    - 包含预测结果的字典
    """
    
    if classifier_model is None:
        from ml.active_learning.classifier.XGBoostClassifier import XGBoostClassifier
        classifier_model = XGBoostClassifier
    
    # 加载模型
    print "Loading model from " + model_path
    with open(model_path, 'rb') as f:
        classifier = pickle.load(f)
    
    # 为新数据准备特征
    print "Preparing features for new data..."
    test_indices = np.arange(new_dataSet.shape[0])
    train_indices = np.arange(new_dataSet.shape[0])  # 使用所有数据作为"训练"索引
    
    # 创建特征矩阵
    all_matrix_test, _, feature_name_list = create_features_matrix(
        new_dataSet, train_indices, test_indices)
    
    try:
        feature_matrix_test = all_matrix_test.tocsr()
    except:
        feature_matrix_test = all_matrix_test

    # 为每列创建特征矩阵并添加列标识
    feature_matrix_test_per_column = []
    test_debugging_ids = []
    
    for column_i in xrange(new_dataSet.shape[1]):
        # 添加列标识的one-hot编码
        one_hot_part = np.zeros((new_dataSet.shape[0], new_dataSet.shape[1]))
        one_hot_part[:, column_i] = 1
        
        # 将列标识特征与原有特征合并
        feature_matrix_test_per_column.append(hstack((feature_matrix_test, one_hot_part)).tocsr())
        
        # 记录调试信息
        for row in xrange(new_dataSet.shape[0]):
            test_debugging_ids.append((row, column_i))

    # 垂直堆叠所有列的特征矩阵
    all_columns_feature_matrix_test = vstack(feature_matrix_test_per_column)
    
    print "Test feature matrix shape: " + str(all_columns_feature_matrix_test.shape)
    
        # 在新数据上进行预测
    print "Making predictions on new data..."
    probability_prediction = classifier.predict_proba(all_columns_feature_matrix_test)
    class_prediction = classifier.predict(all_columns_feature_matrix_test)
    
    # 按列组织预测结果
    per_column_predictions = {}
    per_column_probabilities = {}
    
    for col_i in xrange(new_dataSet.shape[1]):
        start_idx = col_i * new_dataSet.shape[0]
        end_idx = (col_i + 1) * new_dataSet.shape[0]
        
        col_pred = class_prediction[start_idx:end_idx]
        col_prob = probability_prediction[start_idx:end_idx]
        
        per_column_predictions[new_dataSet.clean_pd.columns[col_i]] = col_pred
        per_column_probabilities[new_dataSet.clean_pd.columns[col_i]] = col_prob
    
    # 返回结果
    result = {
        'predictions': class_prediction,
        'probabilities': probability_prediction,
        'per_column_predictions': per_column_predictions,
        'per_column_probabilities': per_column_probabilities,
        'debugging_ids': test_debugging_ids
    }
    
    return result


def create_simplified_features(dataSet, train_indices, test_indices, 
                             use_metadata=False, use_metadata_only=False,
                             use_word2vec=False, use_word2vec_only=False, w2v_size=100,
                             use_active_clean=False, use_activeclean_only=False,
                             use_boostclean_metadata=False, use_boostclean_metadata_only=False):
    """
    创建简化版特征矩阵
    """
    # 基础字符级特征
    all_matrix_train, all_matrix_test, feature_name_list = create_features(
        dataSet, train_indices, test_indices, ngrams=1, runSVD=False, is_word=False, use_tf_idf=True)

    # 元数据特征
    if use_metadata:
        all_matrix_train, all_matrix_test, feature_name_list = add_metadata_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_metadata_only)

    # Word2Vec特征
    if use_word2vec:
        w2v_features = Word2VecFeatures(vector_size=w2v_size)
        all_matrix_train, all_matrix_test, feature_name_list = w2v_features.add_word2vec_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_word2vec_only)

    # ActiveClean特征
    if use_active_clean:
        ac_features = ActiveCleanFeatures()
        all_matrix_train, all_matrix_test, feature_name_list = ac_features.add_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_activeclean_only)

    # BoostClean元数据特征
    if use_boostclean_metadata:
        ac_features = BoostCleanMetaFeatures()
        all_matrix_train, all_matrix_test, feature_name_list = ac_features.add_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_boostclean_metadata_only)

    return all_matrix_train, all_matrix_test, feature_name_list


def run_simplified_experiment(train_dataSet, test_dataSet=None, classifier_model=None,
                            feature_config=None, train_fraction=0.8):
    """
    运行简化版错误检测实验
    
    参数:
    - train_dataSet: 训练数据集
    - test_dataSet: 测试数据集
    - classifier_model: 分类器模型
    - feature_config: 特征配置字典
    - train_fraction: 训练数据比例
    """
    
    if feature_config is None:
        feature_config = {
            'use_metadata': True,
            'use_word2vec': False,
            'use_active_clean': False,
            'use_boostclean_metadata': False
        }
    
    if classifier_model is None:
        classifier_model = SimplifiedXGBoostClassifier
    
    print "Running simplified experiment on " + train_dataSet.name
    if test_dataSet:
        print "Using separate test dataset: " + test_dataSet.name
    
    # 训练和评估模型
    result = train_and_evaluate_error_detection_model(
        train_dataSet=train_dataSet,
        test_dataSet=test_dataSet,
        classifier_model=classifier_model,
        train_fraction=train_fraction,
        **feature_config
    )
    
    return result


def batch_run_experiments(datasets, classifiers, feature_configs):
    """
    批量运行多个实验
    
    参数:
    - datasets: 数据集列表
    - classifiers: 分类器列表
    - feature_configs: 特征配置列表
    """
    
    results = []
    
    for dataset in datasets:
        data = dataset()
        for classifier in classifiers:
            for config in feature_configs:
                try:
                    print "Running experiment: " + data.name + " with " + classifier.__name__ + " and config " + str(config)
                    result = run_simplified_experiment(
                        train_dataSet=data,
                        classifier_model=classifier,
                        feature_config=config
                    )
                    result['dataset_name'] = data.name
                    result['classifier_name'] = classifier.__name__
                    result['config'] = config
                    results.append(result)
                    
                    print "Completed: F1={0:.4f}".format(result['overall']['f1'])
                    
                except Exception as e:
                    print "Error in experiment: " + str(e)
                    results.append({
                        'error': str(e),
                        'dataset_name': data.name,
                        'classifier_name': classifier.__name__,
                        'config': config
                    })
    
    return results


# 示例用法
if __name__ == "__main__":
    from ml.datasets.adult.Adult import Adult
    from ml.active_learning.classifier.XGBoostClassifier import XGBoostClassifier
    from ml.active_learning.classifier.LinearSVMClassifier import LinearSVMClassifier
    
    # 示例：运行单个实验
    adult_data = Adult()
    result = run_simplified_experiment(
        train_dataSet=adult_data,
        classifier_model=XGBoostClassifier,
        feature_config={
            'use_metadata': True,
            'use_word2vec': True,
            'w2v_size': 100
        }
    )
    
    print "Experiment completed with F1 score: {0:.4f}".format(result['overall']['f1'])
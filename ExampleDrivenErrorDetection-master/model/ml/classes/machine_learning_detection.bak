# -*- coding: utf-8 -*-
# simplified_error_detection.py
import numpy as np
from sklearn.metrics import f1_score, precision_score, recall_score
from ml.active_learning.library import *
from ml.Word2VecFeatures.Word2VecFeatures import Word2VecFeatures
from ml.features.ActiveCleanFeatures import ActiveCleanFeatures
from ml.features.ValueCorrelationFeatures import ValueCorrelationFeatures
from ml.features.BoostCleanMetaFeatures import BoostCleanMetaFeatures 

def add_lstm_features(data, use_lstm_only, all_matrix_train, feature_name_list):
    lstm_path = ""
    if data.name == 'Flight HoloClean':
        lstm_path = Config.get('lstm.folder') + "/Flights/last/"
    elif data.name == 'HospitalHoloClean':
        lstm_path = Config.get('lstm.folder') + "/HospitalHoloClean/last/"
    elif data.name == 'BlackOakUppercase':
        lstm_path = Config.get('lstm.folder') + "/BlackOakUppercase/last/"
    else:
        raise Exception('We have no potential model for this dataset yet')

    all_matrix_train_deep = read_compressed_deep_features(lstm_path)
    all_matrix_test = None
    feature_name_list_deep = ['deep ' + str(dfeature) for dfeature in xrange(all_matrix_train_deep.shape[1])]

    if use_lstm_only:
        all_matrix_train = all_matrix_train_deep
        feature_name_list = feature_name_list_deep
    else:
        all_matrix_train = hstack((all_matrix_train, all_matrix_train_deep)).tocsr()
        feature_name_list.extend(feature_name_list_deep)

    return all_matrix_train, all_matrix_test, feature_name_list

def create_features_matrix(dataSet, train_indices, test_indices, 
                          ngrams=1, runSVD=False, is_word=False, use_tf_idf=True,
                          use_metadata=True, use_metadata_only=False,
                          use_lstm=False, use_lstm_only=False,
                          use_word2vec=False, use_word2vec_only=False, w2v_size=100,
                          use_active_clean=False, use_activeclean_only=False,
                          use_cond_prob=False, use_cond_prob_only=False,
                          use_boostclean_metadata=False, use_boostclean_metadata_only=False):
    """
    创建特征矩阵
    """
    # 基础特征
    all_matrix_train, all_matrix_test, feature_name_list = create_features(
        dataSet, train_indices, test_indices, ngrams, runSVD, is_word, use_tf_idf)

    # 元数据特征
    if use_metadata:
        all_matrix_train, all_matrix_test, feature_name_list = add_metadata_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_metadata_only)

    # LSTM特征
    if use_lstm:
        all_matrix_train, all_matrix_test, feature_name_list = add_lstm_features(
            dataSet, use_lstm_only, all_matrix_train, feature_name_list)

    # Word2Vec特征
    if use_word2vec:
        w2v_features = Word2VecFeatures(vector_size=w2v_size)
        all_matrix_train, all_matrix_test, feature_name_list = w2v_features.add_word2vec_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_word2vec_only)

    # ActiveClean特征
    if use_active_clean:
        ac_features = ActiveCleanFeatures()
        all_matrix_train, all_matrix_test, feature_name_list = ac_features.add_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_activeclean_only)

    # 条件概率特征
    if use_cond_prob:
        ac_features = ValueCorrelationFeatures()
        all_matrix_train, all_matrix_test, feature_name_list = ac_features.add_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_cond_prob_only)

    # BoostClean元数据特征
    if use_boostclean_metadata:
        ac_features = BoostCleanMetaFeatures()
        all_matrix_train, all_matrix_test, feature_name_list = ac_features.add_features(
            dataSet, train_indices, test_indices, all_matrix_train,
            all_matrix_test, feature_name_list, use_boostclean_metadata_only)

    return all_matrix_train, all_matrix_test, feature_name_list

def prepare_data_for_training_with_split_datasets(train_dataSet, test_dataSet):
    """
    为训练准备数据，使用已分割的训练集和测试集
    
    参数:
    - train_dataSet: 训练数据集对象
    - test_dataSet: 测试数据集对象
    """
    
    # 创建训练集特征矩阵
    train_indices = np.arange(train_dataSet.shape[0])
    test_indices = np.arange(test_dataSet.shape[0])
    
    # 为训练集创建特征
    all_matrix_train, _, feature_name_list = create_features_matrix(
        train_dataSet, train_indices, test_indices)
    
    try:
        feature_matrix_train = all_matrix_train.tocsr()
    except:
        feature_matrix_train = all_matrix_train

    # 为测试集创建特征
    all_matrix_test, _, _ = create_features_matrix(
        test_dataSet, train_indices, test_indices)
    
    try:
        feature_matrix_test = all_matrix_test.tocsr()
    except:
        feature_matrix_test = all_matrix_test

    # 为训练集每列创建特征矩阵并添加列标识
    feature_matrix_train_per_column = []
    train_debugging_ids = []
    
    for column_i in xrange(train_dataSet.shape[1]):
        # 添加列标识的one-hot编码
        one_hot_part = np.zeros((train_dataSet.shape[0], train_dataSet.shape[1]))
        one_hot_part[:, column_i] = 1
        
        # 将列标识特征与原有特征合并
        feature_matrix_train_per_column.append(hstack((feature_matrix_train, one_hot_part)).tocsr())
        
        # 记录调试信息
        for row in xrange(train_dataSet.shape[0]):
            train_debugging_ids.append((row, column_i))

    # 垂直堆叠训练集所有列的特征矩阵
    all_columns_feature_matrix_train = vstack(feature_matrix_train_per_column)
    
    # 为测试集每列创建特征矩阵并添加列标识
    feature_matrix_test_per_column = []
    test_debugging_ids = []
    
    for column_i in xrange(test_dataSet.shape[1]):
        # 添加列标识的one-hot编码
        one_hot_part = np.zeros((test_dataSet.shape[0], test_dataSet.shape[1]))
        one_hot_part[:, column_i] = 1
        
        # 将列标识特征与原有特征合并
        feature_matrix_test_per_column.append(hstack((feature_matrix_test, one_hot_part)).tocsr())
        
        # 记录调试信息
        for row in xrange(test_dataSet.shape[0]):
            test_debugging_ids.append((row, column_i))

    # 垂直堆叠测试集所有列的特征矩阵
    all_columns_feature_matrix_test = vstack(feature_matrix_test_per_column)
    
    # 创建训练集标签数组（将所有列的标签展平）
    ground_truth_train_array = train_dataSet.matrix_is_error[:, 0]
    for column_i in xrange(1, train_dataSet.shape[1]):
        ground_truth_train_array = np.concatenate((ground_truth_train_array, train_dataSet.matrix_is_error[:, column_i]))

    # 创建测试集标签数组（将所有列的标签展平）
    ground_truth_test_array = test_dataSet.matrix_is_error[:, 0]
    for column_i in xrange(1, test_dataSet.shape[1]):
        ground_truth_test_array = np.concatenate((ground_truth_test_array, test_dataSet.matrix_is_error[:, column_i]))

    return (all_columns_feature_matrix_train, all_columns_feature_matrix_test, 
            ground_truth_train_array, ground_truth_test_array, 
            feature_name_list, train_debugging_ids, test_debugging_ids)

def train_and_evaluate_error_detection_model_with_split_datasets(train_dataSet, test_dataSet, classifier_model,
                                           ngrams=1, runSVD=False, is_word=False,
                                           use_metadata=True, use_metadata_only=False,
                                           use_lstm=False, use_lstm_only=False,
                                           use_word2vec=False, use_word2vec_only=False, w2v_size=100,
                                           use_active_clean=False, use_activeclean_only=False,
                                           use_cond_prob=False, use_cond_prob_only=False,
                                           use_boostclean_metadata=False, use_boostclean_metadata_only=False):
    """
    简化版错误检测模型训练和评估，使用已分割的训练集和测试集
    
    参数:
    - train_dataSet: 训练数据集对象
    - test_dataSet: 测试数据集对象
    - classifier_model: 分类器模型类
    - 各种特征开关参数
    
    返回:
    - 包含评估指标的字典
    """
    
    # 准备训练数据
    print "Preparing training data..."
    (all_columns_feature_matrix_train, all_columns_feature_matrix_test, 
     ground_truth_train_array, ground_truth_test_array, 
     feature_name_list, train_debugging_ids, test_debugging_ids) = prepare_data_for_training_with_split_datasets(
        train_dataSet, test_dataSet)
    
    print "Train feature matrix shape: " + str(all_columns_feature_matrix_train.shape)
    print "Test feature matrix shape: " + str(all_columns_feature_matrix_test.shape)
    print "Train ground truth array length: " + str(len(ground_truth_train_array))
    print "Test ground truth array length: " + str(len(ground_truth_test_array))
    
    # 初始化分类器
    classifier = classifier_model(all_columns_feature_matrix_train, None, feature_names=feature_name_list)
    
    # 训练模型
    print "Training model..."
    # 使用训练集训练模型
    classifier.train(all_columns_feature_matrix_train, ground_truth_train_array)
    
    # 在测试集上进行预测
    print "Making predictions on test set..."
    probability_prediction = classifier.predict_proba(all_columns_feature_matrix_test)
    class_prediction = classifier.predict(all_columns_feature_matrix_test)
    
    # 计算评估指标
    f1 = f1_score(ground_truth_test_array, class_prediction)
    precision = precision_score(ground_truth_test_array, class_prediction)
    recall = recall_score(ground_truth_test_array, class_prediction)
    
    # 按列计算评估指标
    per_column_metrics = {}
    for col_i in xrange(test_dataSet.shape[1]):
        start_idx = col_i * test_dataSet.shape[0]
        end_idx = (col_i + 1) * test_dataSet.shape[0]
        
        col_true = ground_truth_test_array[start_idx:end_idx]
        col_pred = class_prediction[start_idx:end_idx]
        
        per_column_metrics[test_dataSet.clean_pd.columns[col_i]] = {
            'f1': f1_score(col_true, col_pred),
            'precision': precision_score(col_true, col_pred),
            'recall': recall_score(col_true, col_pred)
        }
    
    # 返回结果
    result = {
        'overall': {
            'f1': f1,
            'precision': precision,
            'recall': recall,
            'total_train_samples': len(ground_truth_train_array),
            'total_test_samples': len(ground_truth_test_array)
        },
        'per_column': per_column_metrics,
        'predictions': class_prediction,
        'ground_truth': ground_truth_test_array,
        'probabilities': probability_prediction
    }
    
    return result

def run_error_detection_experiment_with_split_datasets(train_dataSet, test_dataSet, classifier_model, **kwargs):
    """
    运行错误检测实验，使用已分割的训练集和测试集
    
    参数:
    - train_dataSet: 训练数据集对象
    - test_dataSet: 测试数据集对象
    - classifier_model: 分类器模型类
    - **kwargs: 其他参数传递给train_and_evaluate_error_detection_model_with_split_datasets
    """
    
    print "Running error detection experiment on " + train_dataSet.name + " (train) and " + test_dataSet.name + " (test)"
    print "=" * 50
    
    # 训练和评估模型
    result = train_and_evaluate_error_detection_model_with_split_datasets(
        train_dataSet, test_dataSet, classifier_model, **kwargs)
    
    # 打印结果
    print "\nOverall Results:"
    print "F1 Score: {0:.4f}".format(result['overall']['f1'])
    print "Precision: {0:.4f}".format(result['overall']['precision'])
    print "Recall: {0:.4f}".format(result['overall']['recall'])
    print "Total Train Samples: " + str(result['overall']['total_train_samples'])
    print "Total Test Samples: " + str(result['overall']['total_test_samples'])
    
    print "\nPer-Column Results:"
    for col_name, metrics in result['per_column'].items():
        print col_name + ": F1={0:.4f}, Precision={1:.4f}, Recall={2:.4f}".format(metrics['f1'], metrics['precision'], metrics['recall'])
    
    return result

# 使用示例:
"""
from ml.datasets.adult.Adult import Adult
from ml.active_learning.classifier.XGBoostClassifier import XGBoostClassifier

# 加载训练集和测试集数据集
train_data = Adult()  # 假设这是从train.csv加载的
test_data = Adult()   # 假设这是从test.csv加载的

# 运行实验
result = run_error_detection_experiment_with_split_datasets(
    train_dataSet=train_data,
    test_dataSet=test_data,
    classifier_model=XGBoostClassifier,
    use_metadata=True,
    use_word2vec=True,
    w2v_size=100
)
"""